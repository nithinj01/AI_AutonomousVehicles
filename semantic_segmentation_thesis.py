# -*- coding: utf-8 -*-
"""Semantic_Segmentation_thesis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wttD6uVhHPJXj2jURCs4Jft_rc4A7_Hz
"""

from google.colab import drive
drive.mount('/content/drive')

"""This is a paper summary of the paper:
ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation
by Adam Paszke
Paper: https://arxiv.org/abs/1606.02147
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/Colab Notebooks

"""

*   Road

*   Sidewalk

*   Building

*   Wall

*   Pole

*   TrafficLight

*   TrafficSign
*   Vegetation


*  Terrain


*   Sky


*   Person


*   Rider


*   Car


*   Truck



*   Bus
*   Train


*  Motorcycle

*   Bicycle
*   Unlabeled




"""

import argparse
import cv2
import numpy as np
import imutils
import time

# read the sample input image, resize the image and construct a blob from the sample image

start =  time.time()
SET_WIDTH  =  int(600)

normalize_image= 1/255.0
resize_image_shape=(1024, 512)

sample_img = cv2.imread('images/example_02.jpg')

sample_img = imutils.resize(sample_img, width=SET_WIDTH)
blob_img= cv2.dnn.blobFromImage(sample_img, normalize_image, resize_image_shape,0,swapRB=True, crop=False)
print("[INFO] loading model...")
cv_enet_model = cv2.dnn.readNet('enet-cityscapes/enet-model.net')
cv_enet_model.setInput(blob_img)
cv_enet_model_output = cv_enet_model.forward()

blob_img

label_values = open('enet-cityscapes/enet-classes.txt').read().strip().split("\n")

IMG_OUTPUT_SHAPE_START =1
IMG_OUTPUT_SHAPE_END =4
(classes_num, h, w) = cv_enet_model_output.shape[IMG_OUTPUT_SHAPE_START:IMG_OUTPUT_SHAPE_END]

class_map = np.argmax(cv_enet_model_output[0], axis=0)

import os
if os.path.isfile('enet-cityscapes/enet-colors.txt'):
    CV_ENET_SHAPE_IMG_COLORS = open('enet-cityscapes/enet-colors.txt').read().strip().split("\n")
    CV_ENET_SHAPE_IMG_COLORS = [np.array(c.split(",")).astype("int") for c in CV_ENET_SHAPE_IMG_COLORS]
    CV_ENET_SHAPE_IMG_COLORS = np.array(CV_ENET_SHAPE_IMG_COLORS, dtype="uint8")

else:

    np.random.seed(42)
    CV_ENET_SHAPE_IMG_COLORS = np.random.randint(0, 255, size=(len(label_values) - 1, 3),
                               dtype="uint8")
    CV_ENET_SHAPE_IMG_COLORS = np.vstack([[0, 0, 0], CV_ENET_SHAPE_IMG_COLORS]).astype("uint8")

mask_class_map = CV_ENET_SHAPE_IMG_COLORS[class_map]

mask_class_map = cv2.resize(mask_class_map,(sample_img.shape[1], sample_img.shape[0]), interpolation=cv2.INTER_NEAREST)
class_map = cv2.resize(class_map, (sample_img.shape[1], sample_img.shape[0]), interpolation=cv2.INTER_NEAREST)

cv_enet_model_output = ((0.4*sample_img)+(0.6 * mask_class_map)).astype("uint8")

my_legend = np.zeros(((len(label_values)*25)+25,300,3), dtype="uint8")

for (i, (class_name, img_color)) in enumerate(zip(label_values, CV_ENET_SHAPE_IMG_COLORS)):
    # draw the class name + color on the legend
    color_info = [int(color) for color in img_color]
    cv2.putText(my_legend, class_name, (5, (i * 25) + 17),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
    cv2.rectangle(my_legend, (100, (i * 25)), (300, (i * 25) + 25),
                  tuple(color_info), -1)

from google.colab.patches import cv2_imshow

cv2_imshow(my_legend)
cv2_imshow(sample_img)
cv2_imshow(cv_enet_model_output)
cv2.waitKey(0)
end= time.time()
print("[INFO] inference took {:.4f} seconds".format(end-start))

DEFAULT_FRAME=1
WIDTH = 600

class_labels = open('enet-cityscapes/enet-classes.txt').read().strip().split("\n")

import os
if os.path.isfile('enet-cityscapes/enet-colors.txt'):
    CV_ENET_SHAPE_IMG_COLORS = open('enet-cityscapes/enet-colors.txt').read().strip().split("\n")
    CV_ENET_SHAPE_IMG_COLORS = [np.array(c.split(",")).astype("int") for c in CV_ENET_SHAPE_IMG_COLORS]
    CV_ENET_SHAPE_IMG_COLORS = np.array(CV_ENET_SHAPE_IMG_COLORS, dtype="uint8")

else:

    np.random.seed(42)
    CV_ENET_SHAPE_IMG_COLORS = np.random.randint(0, 255, size=(len(label_values) - 1, 3),
                               dtype="uint8")
    CV_ENET_SHAPE_IMG_COLORS = np.vstack([[0, 0, 0], CV_ENET_SHAPE_IMG_COLORS]).astype("uint8")

print("[INFO] loading model...")
cv_enet_model = cv2.dnn.readNet('enet-cityscapes/enet-model.net')

sample_video =cv2.VideoCapture('video//video.mp4')
sample_video_writer = None

try:
  prop = cv2.cv.CV_CAP_PROP_FRAME_COUNT if imutils.is_cv2() \
    else cv2.CAP_PROP_FRAME_COUNT
  total_time = int(sample_video.get(prop))
  print("[INFO] {} total_time video_frame in video".format(total_time))
except:
  print("[INFO] could not determine number of video_frames in video")
  total_time=-1

total = int(sample_video.get(prop))

total

#sample_video
# loop over frames from the video file stream
while True:
    # read the next frame from the file
    (grabbed, frame) = sample_video.read()

    # if the frame was not grabbed, then we have reached the end
    # of the stream
    if not grabbed:
        break

    # construct a blob from the frame and perform a forward pass
    # using the segmentation model
    normalize_image = 1 / 255.0
    resize_image_shape = (1024, 512)
    video_frame = imutils.resize(frame, width=SET_WIDTH)
    blob_img = cv2.dnn.blobFromImage(frame,  normalize_image,resize_image_shape, 0,
                                 swapRB=True, crop=False)
    cv_enet_model.setInput(blob_img)
    start = time.time()
    cv_enet_model_output = cv_enet_model.forward()
    end = time.time()

    # infer the total number of classes along with the spatial
    # dimensions of the mask image via the shape of the output array
    (Classes_num, height, width) = cv_enet_model_output.shape[1:4]

    # our output class ID map will be num_classes x height x width in
    # size, so we take the argmax to find the class label with the
    # largest probability for each and every (x, y)-coordinate in the
    # image
    classMap = np.argmax(cv_enet_model_output[0], axis=0)

    # given the class ID map, we can map each of the class IDs to its
    # corresponding color

    mask_class_map = CV_ENET_SHAPE_IMG_COLORS[classMap]

    # resize the mask such that its dimensions match the original size
    # of the input frame


    mask_class_map = cv2.resize(mask_class_map, (video_frame.shape[1], video_frame.shape[0]),
                      interpolation=cv2.INTER_NEAREST)

    # perform a weighted combination of the input frame with the mask
    # to form an output visualization


    cv_enet_model_output = ((0.3 * video_frame) + (0.7 * mask_class_map)).astype("uint8")

    # check if the video writer is None
    if sample_video_writer is None:
        print("sample_video_writer is None")
        # initialize our video writer
        fourcc_obj = cv2.VideoWriter_fourcc(*"MJPG")

        sample_video_writer = cv2.VideoWriter('./output/output_toronoto.avi', fourcc_obj, 30,
                                 (cv_enet_model_output.shape[1], cv_enet_model_output.shape[0]), True)

        # some information on processing single frame
        if total > 0:

            execution_time = (end - start)
            print("[INFO] single video_frame took {:.4f} seconds".format(execution_time))

            print("[INFO] estimated total_time time: {:.4f}".format(
                execution_time * total))

    # write the output frame to disk

    sample_video_writer.write(cv_enet_model_output)

    # check to see if we should display the output frame to our screen
    if DEFAULT_FRAME > 0:
        cv2_imshow(cv_enet_model_output)

        key = cv2.waitKey(1) & 0xFF

        if key == ord("q"):
            break

print("cleaning up...")
sample_video_writer.release()
sv.release()